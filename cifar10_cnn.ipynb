{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.optimizers import RMSprop, Adam, Adagrad, SGD\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 3\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn(conv_blocks_filters=[32, 64], cnn_dropout=0.25,\n",
    "            conv_filter_size=(3, 3),\n",
    "            pool_type='max', pool_size=(2, 2), activation='relu',\n",
    "            clf_layers=[512, 128, 64], clf_dropout=0.5,\n",
    "            optimizer='adam', learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Function that compiles basic convolutional neural network\n",
    "    Args:\n",
    "        conv_blocks_filters: number of filters for each block (conv, conv, pool)\n",
    "        cnn_dropout: dropout after the cnn block\n",
    "        conv_filter_size: size of convolutional filters\n",
    "        pool_type: type of pooling. Could be \"max\" or \"average\"\n",
    "        pool_size: size of pooling\n",
    "        activation: name of the activation function\n",
    "        clf_layers: list with neuron number for each layer of the classifier neural network\n",
    "        clf_dropout: dropout rate in the classifier(clf) nn\n",
    "        optimizer: name of optimizer for the neural network\n",
    "        learning_rate: learning rate for the optimizer\n",
    "\n",
    "    Returns:\n",
    "        compiled model\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for cnn_filters in conv_blocks_filters:\n",
    "        model.add(Conv2D(cnn_filters, conv_filter_size,\n",
    "                         padding='same'))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        model.add(Conv2D(cnn_filters, conv_filter_size))\n",
    "        model.add(Activation(activation))\n",
    "\n",
    "        if pool_type == 'max':\n",
    "            model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        elif pool_type == 'average':\n",
    "            model.add(AveragePooling2D(pool_size=pool_size))\n",
    "        else:\n",
    "            raise ValueError(f\"pool_type {pool_type} not recognized\")\n",
    "        model.add(Dropout(cnn_dropout))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for neuron_num in clf_layers:\n",
    "        model.add(Dense(neuron_num))\n",
    "        model.add(Activation(activation))\n",
    "        model.add(Dropout(clf_dropout))\n",
    "\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Selecting optimizer\n",
    "    if optimizer == 'rmsprop':\n",
    "        opt = RMSprop(lr=learning_rate)\n",
    "    elif optimizer == 'adam':\n",
    "        opt = Adam(lr=learning_rate)\n",
    "    elif optimizer == 'adamgrad':\n",
    "        opt = Adagrad(lr=learning_rate)\n",
    "    elif optimizer == 'sgd':\n",
    "        opt = SGD(lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"optimizer {optimizer} not recognized\")\n",
    "\n",
    "    # Let's train the model using optimizer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pool_type': 'max', 'pool_size': (3, 3), 'optimizer': 'sgd', 'learning_rate': 0.001, 'conv_filter_size': (3, 3), 'conv_blocks_filters': [16, 32], 'cnn_dropout': 0.3, 'clf_layers': [512, 64], 'clf_dropout': 0.25, 'activation': 'sigmoid'}\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 120s 2ms/step - loss: 2.3938 - acc: 0.1016 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 119s 2ms/step - loss: 2.3678 - acc: 0.0986 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 118s 2ms/step - loss: 2.3590 - acc: 0.0992 - val_loss: 2.3029 - val_acc: 0.1000\n",
      "Saved trained model at C:\\Users\\User\\Downloads\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 9s 876us/step\n",
      "Test loss: 2.30288709564209\n",
      "Test accuracy: 0.1\n",
      "{'pool_type': 'average', 'pool_size': (3, 3), 'optimizer': 'sgd', 'learning_rate': 0.0001, 'conv_filter_size': (3, 3), 'conv_blocks_filters': [16, 32], 'cnn_dropout': 0.15, 'clf_layers': [512, 64], 'clf_dropout': 0.25, 'activation': 'relu'}\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 119s 2ms/step - loss: 2.3035 - acc: 0.0974 - val_loss: 2.3024 - val_acc: 0.1000\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 117s 2ms/step - loss: 2.3030 - acc: 0.0992 - val_loss: 2.3017 - val_acc: 0.1003\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 117s 2ms/step - loss: 2.3022 - acc: 0.1010 - val_loss: 2.3010 - val_acc: 0.0999\n",
      "Saved trained model at C:\\Users\\User\\Downloads\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 8s 832us/step\n",
      "Test loss: 2.300988469696045\n",
      "Test accuracy: 0.0999\n",
      "{'pool_type': 'average', 'pool_size': (3, 3), 'optimizer': 'sgd', 'learning_rate': 0.001, 'conv_filter_size': (3, 3), 'conv_blocks_filters': [16, 32], 'cnn_dropout': 0.15, 'clf_layers': [512, 64], 'clf_dropout': 0.5, 'activation': 'relu'}\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 118s 2ms/step - loss: 2.3030 - acc: 0.0993 - val_loss: 2.3011 - val_acc: 0.1223\n",
      "Epoch 2/3\n",
      "50000/50000 [==============================] - 116s 2ms/step - loss: 2.3016 - acc: 0.1077 - val_loss: 2.2996 - val_acc: 0.1563\n",
      "Epoch 3/3\n",
      "50000/50000 [==============================] - 115s 2ms/step - loss: 2.3000 - acc: 0.1187 - val_loss: 2.2973 - val_acc: 0.1838\n",
      "Saved trained model at C:\\Users\\User\\Downloads\\saved_models\\keras_cifar10_trained_model.h5 \n",
      "10000/10000 [==============================] - 8s 819us/step\n",
      "Test loss: 2.2972782283782958\n",
      "Test accuracy: 0.1838\n",
      "{'pool_type': 'max', 'pool_size': (3, 3), 'optimizer': 'sgd', 'learning_rate': 0.01, 'conv_filter_size': (3, 3), 'conv_blocks_filters': [16, 32], 'cnn_dropout': 0.15, 'clf_layers': [512, 64], 'clf_dropout': 0.25, 'activation': 'sigmoid'}\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "50000/50000 [==============================] - 147s 3ms/step - loss: 2.3420 - acc: 0.1000 - val_loss: 2.3032 - val_acc: 0.1000\n",
      "Epoch 2/3\n",
      " 1696/50000 [>.............................] - ETA: 2:15 - loss: 2.3112 - acc: 0.1126"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f52959df1a5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m               shuffle=True)\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m# Save model and weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programs\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Programs\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programs\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Programs\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperspace_grid = {'conv_blocks_filters':[ [32, 64], [16, 32]],\n",
    "                   'cnn_dropout': [0.15, 0.3],\n",
    "                   'conv_filter_size': [(3,3), (2,2)],\n",
    "                   'pool_type': ['max', 'average'],\n",
    "                   'pool_size': [(3,3), (2,2)],\n",
    "                   'activation': ['relu', 'sigmoid'],\n",
    "                   'clf_layers': [ [512], [512, 128, 64]],\n",
    "                   'clf_dropout': [0.25, 0.5],\n",
    "                   'optimizer': ['adam', 'sgd'],\n",
    "                   'learning_rate': [0.0001, 0.001, 0.01]\n",
    "                  }\n",
    "\n",
    "# We are taking 10 random configurations from this grid. NOT ALL!\n",
    "n_configurations = 10\n",
    "configurations_list = list(ParameterSampler(hyperspace_grid,\n",
    "                                            n_iter=n_configurations))\n",
    "\n",
    "configurations_loss = []\n",
    "configurations_accuracy = []\n",
    "\n",
    "for configuration in configurations_list:\n",
    "    print(configuration)\n",
    "    model = get_cnn(**configuration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)\n",
    "\n",
    "    # Save model and weights\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    model_path = os.path.join(save_dir, model_name)\n",
    "    model.save(model_path)\n",
    "    print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "    # Score trained model.\n",
    "    scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "    configurations_loss.append(scores[0])\n",
    "    configurations_accuracy.append(scores[1])\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    \n",
    "results = pd.DataFrame({'loss':configurations_loss,\n",
    "                        'accuracy': configurations_accuracy,\n",
    "                        'params': str(configuration)})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
